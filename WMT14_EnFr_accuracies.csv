RANK,METHOD,"BLEU SCORE",SACREBLEU,"PAPER TITLE",YEAR,PAPER,CODE
1,"Transformer Big + BT",45.6,43.8,"Understanding Back-Translation at Scale",2018,,
2,"Local Joint Self-attention",43.3,,"Joint Source-Target Self Attention with Locality Constraints",2019,,
3,"depth growing",43.27,,"Depth Growing for Neural Machine Translation",2019,,
4,"Transformer Big",43.2,,"Scaling Neural Machine Translation",2018,,
5,DynamicConv,43.2,,"Pay Less Attention with Lightweight and Dynamic Convolutions",2019,,
6,LightConv,43.1,,"Pay Less Attention with Lightweight and Dynamic Convolutions",2019,,
7,"Transformer Big + MoS",42.1,,"Fast and Simple Mixture of Softmaxes with BPE and Hybrid-LightRNN for Language Generation",2018,,
8,"Transformer (big)",41.5,,"Self-Attention with Relative Position Representations",2018,,
9,"Weighted Transformer (large)",41.4,,"Weighted Transformer Network for Machine Translation",2017,,
10,"ConvS2S (ensemble)",41.3,,"Convolutional Sequence to Sequence Learning",2017,,
11,"Evolved Transformer Big",41.3,,"The Evolved Transformer",2019,,
12,RNMT+,41,,"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",2018,,
13,"Transformer Big",41,,"Attention Is All You Need",2017,,
14,"Evolved Transformer Base",40.6,,"The Evolved Transformer",2019,,
15,MoE,40.56,,"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",2017,,
16,ConvS2S,40.46,,"Convolutional Sequence to Sequence Learning",2017,,
17,GNMT+RL,39.9,,"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",2016,,
18,"Deep-Att + PosUnk",39.2,,"Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",2016,,
19,"Transformer Base",38.1,,"Attention Is All You Need",2017,,
20,"LSTM6 + PosUnk",37.5,,"Addressing the Rare Word Problem in Neural Machine Translation",2014,,
21,"PBMT ()",37,,,,,
22,SMT+LSTM5,36.5,,"Sequence to Sequence Learning with Neural Networks",2014,,
23,RNN-search50*,36.2,,"Neural Machine Translation by Jointly Learning to Align and Translate",2014,,
24,Deep-Att,35.9,,"Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",2016,,
25,"Deep Convolutional Encoder; single-layer decoder",35.7,,"A Convolutional Encoder Model for Neural Machine Translation",2016,,
26,LSTM,34.8,,"Sequence to Sequence Learning with Neural Networks",2014,,
27,"CSLM + RNN + WP",34.54,,"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",2014,,
28,"Regularized LSTM",29.03,,"Recurrent Neural Network Regularization",2014,,
29,"Unsupervised PBSMT",28.11,,"Phrase-Based & Neural Unsupervised Machine Translation",2018,,
30,"PBSMT + NMT",27.6,,"Phrase-Based & Neural Unsupervised Machine Translation",2018,,
31,GRU+Attention,26.4,,"Can Active Memory Replace Attention?",2016,,
32,"SMT + iterative backtranslation (unsupervised)",26.22,,"Unsupervised Statistical Machine Translation",2018,,
33,"Unsupervised NMT + Transformer",25.14,,"Phrase-Based & Neural Unsupervised Machine Translation",2018,,
34,ByteNet,23.8,,"Neural Machine Translation in Linear Time",2016,,
35,"Unsupervised attentional encoder-decoder + BPE",14.36,,"Unsupervised Neural Machine Translation",2017,,
